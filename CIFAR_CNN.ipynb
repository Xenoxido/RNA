{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpAaRoszLqY",
        "colab_type": "code",
        "outputId": "43cf4c5d-a47f-4a2c-d0bc-5d81ecb8a67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "num_classes = 10\n",
        "epochs = 150\n",
        "\n",
        "\n",
        "#### LOAD AND TRANSFORM\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#Modificado: DATA AUGMENTATION\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    rotation_range=20,\n",
        "    zoom_range=[1.0,1.2],\n",
        "    horizontal_flip=True)\n",
        "\n",
        "## DEF A BLOCK CONV + BN + GN + MAXPOOL\n",
        "def CBGN(model,filters,ishape=0):\n",
        "  if (ishape!=0):\n",
        "    model.add(Conv2D(filters, (3, 3), padding='same',\n",
        "                 input_shape=ishape))\n",
        "  else:\n",
        "    model.add(Conv2D(filters, (3, 3), padding='same'))\n",
        "  \n",
        "  #Modificado: Batch-norm despues de funcion de activacion\n",
        "  #Modificado: Gaussian-Noise = 0.1\n",
        "  #Modificado: Añadida un filtro mas\n",
        "  #Modificado: Se añade Dropout = 0.1. No ha ido bien.\n",
        "  #modificado: Añado un segunda filtro\n",
        "  model.add(GN(0.1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BN())\n",
        "\n",
        "  model.add(Conv2D(filters, (3, 3), padding='same'))\n",
        "  model.add(GN(0.1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BN())\n",
        "\n",
        "  model.add(Conv2D(filters, (3, 3), padding='same'))\n",
        "  model.add(GN(0.1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BN())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  \n",
        "  return model\n",
        "\n",
        "  \n",
        "## DEF NN TOPOLOGY  \n",
        "model = Sequential()\n",
        "\n",
        "model=CBGN(model,32,x_train.shape[1:])\n",
        "model=CBGN(model,64)\n",
        "model=CBGN(model,128)\n",
        "model=CBGN(model,256)\n",
        "model=CBGN(model,512)\n",
        "\n",
        "model.add(Flatten())#Here starts the Neuronal network that recognize 10 diferents types of images\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "## OPTIM AND COMPILE\n",
        "opt = SGD(lr=0.1, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Modificado: Aplico el mismo scheduler de Learning rate annealing que en MNIST\n",
        "def scheduler(epoch):\n",
        "    if epoch < 50:\n",
        "        return .1\n",
        "    elif epoch < 100:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "set_lr = LRS(scheduler)\n",
        "\n",
        "## TRAINING\n",
        "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[set_lr],\n",
        "                            verbose=1)\n",
        "\n",
        "## TEST\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_1 (GaussianNo (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_2 (GaussianNo (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_3 (GaussianNo (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "gaussian_noise_4 (GaussianNo (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "gaussian_noise_5 (GaussianNo (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "gaussian_noise_6 (GaussianNo (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "gaussian_noise_7 (GaussianNo (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "gaussian_noise_8 (GaussianNo (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "gaussian_noise_9 (GaussianNo (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "gaussian_noise_10 (GaussianN (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "gaussian_noise_11 (GaussianN (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "gaussian_noise_12 (GaussianN (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 2, 2, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "gaussian_noise_13 (GaussianN (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "gaussian_noise_14 (GaussianN (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "gaussian_noise_15 (GaussianN (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 8,135,562\n",
            "Trainable params: 8,129,610\n",
            "Non-trainable params: 5,952\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/150\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 1.9208 - acc: 0.3122 - val_loss: 1.8379 - val_acc: 0.3760\n",
            "Epoch 2/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 1.5721 - acc: 0.4308 - val_loss: 1.4085 - val_acc: 0.4908\n",
            "Epoch 3/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 1.3756 - acc: 0.5037 - val_loss: 1.3264 - val_acc: 0.5323\n",
            "Epoch 4/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 1.2321 - acc: 0.5596 - val_loss: 1.2445 - val_acc: 0.5855\n",
            "Epoch 5/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 1.1022 - acc: 0.6087 - val_loss: 0.9516 - val_acc: 0.6658\n",
            "Epoch 6/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 1.0208 - acc: 0.6385 - val_loss: 1.0700 - val_acc: 0.6430\n",
            "Epoch 7/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.9408 - acc: 0.6702 - val_loss: 0.8257 - val_acc: 0.7194\n",
            "Epoch 8/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.8871 - acc: 0.6910 - val_loss: 0.7809 - val_acc: 0.7392\n",
            "Epoch 9/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.8308 - acc: 0.7102 - val_loss: 0.7799 - val_acc: 0.7336\n",
            "Epoch 10/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.7815 - acc: 0.7275 - val_loss: 0.7357 - val_acc: 0.7485\n",
            "Epoch 11/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.7411 - acc: 0.7433 - val_loss: 0.6244 - val_acc: 0.7788\n",
            "Epoch 12/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.7061 - acc: 0.7568 - val_loss: 0.6004 - val_acc: 0.7958\n",
            "Epoch 13/150\n",
            "500/500 [==============================] - 31s 61ms/step - loss: 0.6720 - acc: 0.7681 - val_loss: 0.6910 - val_acc: 0.7679\n",
            "Epoch 14/150\n",
            "500/500 [==============================] - 31s 61ms/step - loss: 0.6498 - acc: 0.7744 - val_loss: 0.5947 - val_acc: 0.7978\n",
            "Epoch 15/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.6235 - acc: 0.7847 - val_loss: 0.5476 - val_acc: 0.8134\n",
            "Epoch 16/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.6031 - acc: 0.7919 - val_loss: 0.5545 - val_acc: 0.8123\n",
            "Epoch 17/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.5878 - acc: 0.7973 - val_loss: 0.5345 - val_acc: 0.8203\n",
            "Epoch 18/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.5710 - acc: 0.8029 - val_loss: 0.4875 - val_acc: 0.8334\n",
            "Epoch 19/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.5541 - acc: 0.8113 - val_loss: 0.5622 - val_acc: 0.8120\n",
            "Epoch 20/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.5369 - acc: 0.8130 - val_loss: 0.5311 - val_acc: 0.8209\n",
            "Epoch 21/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.5221 - acc: 0.8201 - val_loss: 0.4939 - val_acc: 0.8363\n",
            "Epoch 22/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.5080 - acc: 0.8236 - val_loss: 0.4911 - val_acc: 0.8341\n",
            "Epoch 23/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 0.4980 - acc: 0.8284 - val_loss: 0.4873 - val_acc: 0.8364\n",
            "Epoch 24/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 0.4887 - acc: 0.8305 - val_loss: 0.4572 - val_acc: 0.8432\n",
            "Epoch 25/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.4768 - acc: 0.8341 - val_loss: 0.5295 - val_acc: 0.8275\n",
            "Epoch 26/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 0.4631 - acc: 0.8394 - val_loss: 0.5223 - val_acc: 0.8282\n",
            "Epoch 27/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.4553 - acc: 0.8404 - val_loss: 0.5306 - val_acc: 0.8261\n",
            "Epoch 28/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.4586 - acc: 0.8414 - val_loss: 0.4376 - val_acc: 0.8554\n",
            "Epoch 29/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.4407 - acc: 0.8464 - val_loss: 0.4361 - val_acc: 0.8574\n",
            "Epoch 30/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.4304 - acc: 0.8485 - val_loss: 0.4424 - val_acc: 0.8552\n",
            "Epoch 31/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.4212 - acc: 0.8540 - val_loss: 0.4366 - val_acc: 0.8570\n",
            "Epoch 32/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.4115 - acc: 0.8568 - val_loss: 0.4452 - val_acc: 0.8539\n",
            "Epoch 33/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.4064 - acc: 0.8579 - val_loss: 0.4415 - val_acc: 0.8494\n",
            "Epoch 34/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.4021 - acc: 0.8587 - val_loss: 0.4254 - val_acc: 0.8545\n",
            "Epoch 35/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.3932 - acc: 0.8635 - val_loss: 0.4235 - val_acc: 0.8569\n",
            "Epoch 36/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 0.3846 - acc: 0.8665 - val_loss: 0.4180 - val_acc: 0.8616\n",
            "Epoch 37/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.3815 - acc: 0.8678 - val_loss: 0.3888 - val_acc: 0.8697\n",
            "Epoch 38/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.3726 - acc: 0.8711 - val_loss: 0.3870 - val_acc: 0.8737\n",
            "Epoch 39/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.3671 - acc: 0.8710 - val_loss: 0.3998 - val_acc: 0.8663\n",
            "Epoch 40/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.3636 - acc: 0.8734 - val_loss: 0.4175 - val_acc: 0.8595\n",
            "Epoch 41/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.3546 - acc: 0.8766 - val_loss: 0.4005 - val_acc: 0.8682\n",
            "Epoch 42/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.3501 - acc: 0.8777 - val_loss: 0.3750 - val_acc: 0.8743\n",
            "Epoch 43/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 0.3451 - acc: 0.8777 - val_loss: 0.3925 - val_acc: 0.8734\n",
            "Epoch 44/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.3390 - acc: 0.8818 - val_loss: 0.3619 - val_acc: 0.8779\n",
            "Epoch 45/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.3402 - acc: 0.8799 - val_loss: 0.3900 - val_acc: 0.8699\n",
            "Epoch 46/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.3384 - acc: 0.8830 - val_loss: 0.3643 - val_acc: 0.8784\n",
            "Epoch 47/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.3236 - acc: 0.8864 - val_loss: 0.3685 - val_acc: 0.8776\n",
            "Epoch 48/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.3261 - acc: 0.8846 - val_loss: 0.3736 - val_acc: 0.8739\n",
            "Epoch 49/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.3211 - acc: 0.8861 - val_loss: 0.3676 - val_acc: 0.8783\n",
            "Epoch 50/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.3134 - acc: 0.8889 - val_loss: 0.4048 - val_acc: 0.8728\n",
            "Epoch 51/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.2773 - acc: 0.9027 - val_loss: 0.3230 - val_acc: 0.8930\n",
            "Epoch 52/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 0.2585 - acc: 0.9092 - val_loss: 0.3264 - val_acc: 0.8922\n",
            "Epoch 53/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.2534 - acc: 0.9120 - val_loss: 0.3241 - val_acc: 0.8933\n",
            "Epoch 54/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.2476 - acc: 0.9129 - val_loss: 0.3191 - val_acc: 0.8959\n",
            "Epoch 55/150\n",
            "500/500 [==============================] - 32s 64ms/step - loss: 0.2423 - acc: 0.9143 - val_loss: 0.3243 - val_acc: 0.8932\n",
            "Epoch 56/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 0.2381 - acc: 0.9164 - val_loss: 0.3218 - val_acc: 0.8954\n",
            "Epoch 57/150\n",
            "500/500 [==============================] - 32s 63ms/step - loss: 0.2389 - acc: 0.9163 - val_loss: 0.3122 - val_acc: 0.8983\n",
            "Epoch 58/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.2321 - acc: 0.9185 - val_loss: 0.3212 - val_acc: 0.8956\n",
            "Epoch 59/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.2315 - acc: 0.9189 - val_loss: 0.3211 - val_acc: 0.8968\n",
            "Epoch 60/150\n",
            "500/500 [==============================] - 31s 61ms/step - loss: 0.2344 - acc: 0.9175 - val_loss: 0.3217 - val_acc: 0.8967\n",
            "Epoch 61/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.2276 - acc: 0.9192 - val_loss: 0.3149 - val_acc: 0.8992\n",
            "Epoch 62/150\n",
            "500/500 [==============================] - 31s 63ms/step - loss: 0.2323 - acc: 0.9172 - val_loss: 0.3238 - val_acc: 0.8962\n",
            "Epoch 63/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.2255 - acc: 0.9207 - val_loss: 0.3231 - val_acc: 0.8960\n",
            "Epoch 64/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.2243 - acc: 0.9202 - val_loss: 0.3152 - val_acc: 0.8976\n",
            "Epoch 65/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.2222 - acc: 0.9216 - val_loss: 0.3180 - val_acc: 0.8960\n",
            "Epoch 66/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.2228 - acc: 0.9219 - val_loss: 0.3185 - val_acc: 0.8987\n",
            "Epoch 67/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.2196 - acc: 0.9221 - val_loss: 0.3196 - val_acc: 0.8970\n",
            "Epoch 68/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.2167 - acc: 0.9225 - val_loss: 0.3258 - val_acc: 0.8943\n",
            "Epoch 69/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.2186 - acc: 0.9236 - val_loss: 0.3198 - val_acc: 0.8966\n",
            "Epoch 70/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2204 - acc: 0.9232 - val_loss: 0.3142 - val_acc: 0.8999\n",
            "Epoch 71/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2156 - acc: 0.9243 - val_loss: 0.3173 - val_acc: 0.9000\n",
            "Epoch 72/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2160 - acc: 0.9236 - val_loss: 0.3189 - val_acc: 0.8982\n",
            "Epoch 73/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2143 - acc: 0.9239 - val_loss: 0.3245 - val_acc: 0.8966\n",
            "Epoch 74/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2091 - acc: 0.9257 - val_loss: 0.3284 - val_acc: 0.8961\n",
            "Epoch 75/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2125 - acc: 0.9256 - val_loss: 0.3217 - val_acc: 0.8983\n",
            "Epoch 76/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2119 - acc: 0.9246 - val_loss: 0.3255 - val_acc: 0.8964\n",
            "Epoch 77/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2088 - acc: 0.9273 - val_loss: 0.3224 - val_acc: 0.8994\n",
            "Epoch 78/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2055 - acc: 0.9271 - val_loss: 0.3178 - val_acc: 0.9000\n",
            "Epoch 79/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2059 - acc: 0.9262 - val_loss: 0.3278 - val_acc: 0.8952\n",
            "Epoch 80/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2056 - acc: 0.9271 - val_loss: 0.3270 - val_acc: 0.8976\n",
            "Epoch 81/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2055 - acc: 0.9282 - val_loss: 0.3212 - val_acc: 0.8989\n",
            "Epoch 82/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.2070 - acc: 0.9277 - val_loss: 0.3268 - val_acc: 0.8974\n",
            "Epoch 83/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.2061 - acc: 0.9263 - val_loss: 0.3213 - val_acc: 0.8986\n",
            "Epoch 84/150\n",
            "500/500 [==============================] - 31s 61ms/step - loss: 0.2016 - acc: 0.9284 - val_loss: 0.3241 - val_acc: 0.8981\n",
            "Epoch 85/150\n",
            "500/500 [==============================] - 31s 61ms/step - loss: 0.2032 - acc: 0.9279 - val_loss: 0.3236 - val_acc: 0.8986\n",
            "Epoch 86/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.2015 - acc: 0.9288 - val_loss: 0.3280 - val_acc: 0.8976\n",
            "Epoch 87/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1996 - acc: 0.9299 - val_loss: 0.3256 - val_acc: 0.8976\n",
            "Epoch 88/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.2017 - acc: 0.9294 - val_loss: 0.3276 - val_acc: 0.8987\n",
            "Epoch 89/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1968 - acc: 0.9309 - val_loss: 0.3318 - val_acc: 0.8962\n",
            "Epoch 90/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1997 - acc: 0.9287 - val_loss: 0.3247 - val_acc: 0.8993\n",
            "Epoch 91/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.2021 - acc: 0.9289 - val_loss: 0.3229 - val_acc: 0.8985\n",
            "Epoch 92/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1979 - acc: 0.9308 - val_loss: 0.3285 - val_acc: 0.8978\n",
            "Epoch 93/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1954 - acc: 0.9298 - val_loss: 0.3215 - val_acc: 0.9003\n",
            "Epoch 94/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1958 - acc: 0.9319 - val_loss: 0.3220 - val_acc: 0.8994\n",
            "Epoch 95/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1989 - acc: 0.9288 - val_loss: 0.3269 - val_acc: 0.8972\n",
            "Epoch 96/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1913 - acc: 0.9320 - val_loss: 0.3259 - val_acc: 0.8996\n",
            "Epoch 97/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1946 - acc: 0.9300 - val_loss: 0.3317 - val_acc: 0.8977\n",
            "Epoch 98/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1903 - acc: 0.9327 - val_loss: 0.3334 - val_acc: 0.8976\n",
            "Epoch 99/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1953 - acc: 0.9305 - val_loss: 0.3251 - val_acc: 0.9010\n",
            "Epoch 100/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1888 - acc: 0.9324 - val_loss: 0.3312 - val_acc: 0.8974\n",
            "Epoch 101/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1860 - acc: 0.9335 - val_loss: 0.3272 - val_acc: 0.8977\n",
            "Epoch 102/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1899 - acc: 0.9326 - val_loss: 0.3261 - val_acc: 0.8980\n",
            "Epoch 103/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1844 - acc: 0.9358 - val_loss: 0.3253 - val_acc: 0.8985\n",
            "Epoch 104/150\n",
            "500/500 [==============================] - 31s 61ms/step - loss: 0.1827 - acc: 0.9356 - val_loss: 0.3254 - val_acc: 0.8992\n",
            "Epoch 105/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1841 - acc: 0.9351 - val_loss: 0.3271 - val_acc: 0.8988\n",
            "Epoch 106/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1846 - acc: 0.9336 - val_loss: 0.3268 - val_acc: 0.8995\n",
            "Epoch 107/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1818 - acc: 0.9350 - val_loss: 0.3262 - val_acc: 0.9001\n",
            "Epoch 108/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1863 - acc: 0.9331 - val_loss: 0.3264 - val_acc: 0.8995\n",
            "Epoch 109/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1819 - acc: 0.9360 - val_loss: 0.3251 - val_acc: 0.9003\n",
            "Epoch 110/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1826 - acc: 0.9343 - val_loss: 0.3258 - val_acc: 0.9001\n",
            "Epoch 111/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1851 - acc: 0.9344 - val_loss: 0.3269 - val_acc: 0.8996\n",
            "Epoch 112/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1861 - acc: 0.9342 - val_loss: 0.3262 - val_acc: 0.9001\n",
            "Epoch 113/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1841 - acc: 0.9352 - val_loss: 0.3258 - val_acc: 0.9000\n",
            "Epoch 114/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1822 - acc: 0.9347 - val_loss: 0.3270 - val_acc: 0.9004\n",
            "Epoch 115/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1820 - acc: 0.9369 - val_loss: 0.3268 - val_acc: 0.8993\n",
            "Epoch 116/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1822 - acc: 0.9362 - val_loss: 0.3254 - val_acc: 0.8999\n",
            "Epoch 117/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1770 - acc: 0.9377 - val_loss: 0.3253 - val_acc: 0.9000\n",
            "Epoch 118/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1774 - acc: 0.9372 - val_loss: 0.3271 - val_acc: 0.9003\n",
            "Epoch 119/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1808 - acc: 0.9356 - val_loss: 0.3269 - val_acc: 0.9003\n",
            "Epoch 120/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1826 - acc: 0.9340 - val_loss: 0.3270 - val_acc: 0.8998\n",
            "Epoch 121/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1801 - acc: 0.9365 - val_loss: 0.3276 - val_acc: 0.8993\n",
            "Epoch 122/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1852 - acc: 0.9334 - val_loss: 0.3268 - val_acc: 0.8990\n",
            "Epoch 123/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1835 - acc: 0.9346 - val_loss: 0.3247 - val_acc: 0.9007\n",
            "Epoch 124/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1818 - acc: 0.9357 - val_loss: 0.3265 - val_acc: 0.9003\n",
            "Epoch 125/150\n",
            "500/500 [==============================] - 31s 62ms/step - loss: 0.1862 - acc: 0.9335 - val_loss: 0.3243 - val_acc: 0.9011\n",
            "Epoch 126/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1848 - acc: 0.9353 - val_loss: 0.3241 - val_acc: 0.9012\n",
            "Epoch 127/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1803 - acc: 0.9362 - val_loss: 0.3247 - val_acc: 0.9000\n",
            "Epoch 128/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1831 - acc: 0.9341 - val_loss: 0.3261 - val_acc: 0.8999\n",
            "Epoch 129/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1833 - acc: 0.9361 - val_loss: 0.3279 - val_acc: 0.8980\n",
            "Epoch 130/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1829 - acc: 0.9348 - val_loss: 0.3265 - val_acc: 0.8993\n",
            "Epoch 131/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1797 - acc: 0.9358 - val_loss: 0.3257 - val_acc: 0.8996\n",
            "Epoch 132/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1785 - acc: 0.9378 - val_loss: 0.3263 - val_acc: 0.9000\n",
            "Epoch 133/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1820 - acc: 0.9345 - val_loss: 0.3260 - val_acc: 0.9001\n",
            "Epoch 134/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1797 - acc: 0.9359 - val_loss: 0.3256 - val_acc: 0.9005\n",
            "Epoch 135/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1802 - acc: 0.9354 - val_loss: 0.3272 - val_acc: 0.9001\n",
            "Epoch 136/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1812 - acc: 0.9355 - val_loss: 0.3265 - val_acc: 0.9008\n",
            "Epoch 137/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1806 - acc: 0.9353 - val_loss: 0.3277 - val_acc: 0.8995\n",
            "Epoch 138/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1791 - acc: 0.9365 - val_loss: 0.3284 - val_acc: 0.8994\n",
            "Epoch 139/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1773 - acc: 0.9367 - val_loss: 0.3278 - val_acc: 0.9002\n",
            "Epoch 140/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1780 - acc: 0.9372 - val_loss: 0.3282 - val_acc: 0.8995\n",
            "Epoch 141/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1810 - acc: 0.9361 - val_loss: 0.3280 - val_acc: 0.8998\n",
            "Epoch 142/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1808 - acc: 0.9363 - val_loss: 0.3285 - val_acc: 0.8996\n",
            "Epoch 143/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1769 - acc: 0.9368 - val_loss: 0.3283 - val_acc: 0.9004\n",
            "Epoch 144/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1763 - acc: 0.9384 - val_loss: 0.3309 - val_acc: 0.9009\n",
            "Epoch 145/150\n",
            "500/500 [==============================] - 30s 61ms/step - loss: 0.1774 - acc: 0.9358 - val_loss: 0.3296 - val_acc: 0.9011\n",
            "Epoch 146/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1790 - acc: 0.9368 - val_loss: 0.3291 - val_acc: 0.9000\n",
            "Epoch 147/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1764 - acc: 0.9367 - val_loss: 0.3291 - val_acc: 0.9001\n",
            "Epoch 148/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1742 - acc: 0.9381 - val_loss: 0.3278 - val_acc: 0.9011\n",
            "Epoch 149/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1802 - acc: 0.9353 - val_loss: 0.3286 - val_acc: 0.9009\n",
            "Epoch 150/150\n",
            "500/500 [==============================] - 30s 60ms/step - loss: 0.1778 - acc: 0.9363 - val_loss: 0.3277 - val_acc: 0.9010\n",
            "10000/10000 [==============================] - 2s 224us/step\n",
            "Test loss: 0.3277245296806097\n",
            "Test accuracy: 0.901\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}