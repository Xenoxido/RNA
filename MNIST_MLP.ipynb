{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "doXKW397GkBl",
        "colab_type": "code",
        "outputId": "644a0ffb-32ef-41e0-d10e-66814030a90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "num_classes = 10\n",
        "epochs = 75\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "# Mandatory to use ImageDataGenerator, it expects 4D Tensors\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "## Data Augmentation with an ImageGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "\n",
        "## Model, note the reshape\n",
        "model = Sequential()\n",
        "model.add(Reshape(target_shape=(784,), input_shape=(28,28,1)))\n",
        "model.add(GN(0.1))\n",
        "\n",
        "# Modificado: Batch-norm despues de la funcion de activaci√≥n\n",
        "# Modificado: Gaussian-Noise = 0.1\n",
        "model.add(Dense(1024))\n",
        "model.add(GN(0.1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(GN(0.1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(GN(0.1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "##\n",
        "\n",
        "sgd=SGD(lr=0.1, decay=0.0, momentum=0.0)\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 25:\n",
        "        return .1\n",
        "    elif epoch < 50:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "#Modificado: Schedule para el Learning rate Annealing con Decrease+Restart learning rate. Desechado.\n",
        "def schedulerRestarDecrease(epoch):\n",
        "    if epoch < 12:\n",
        "        return .1\n",
        "    elif epoch < 25:\n",
        "        return 0.01\n",
        "    elif epoch < 37:\n",
        "        return 0.001\n",
        "    elif epoch < 50:\n",
        "        return 0.1\n",
        "    elif epoch < 62:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "\n",
        "set_lr = LRS(scheduler)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[set_lr],\n",
        "                            verbose=1)\n",
        "\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_4 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "gaussian_noise_13 (GaussianN (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "gaussian_noise_14 (GaussianN (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "gaussian_noise_15 (GaussianN (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "gaussian_noise_16 (GaussianN (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n",
            "Epoch 1/75\n",
            "600/600 [==============================] - 23s 39ms/step - loss: 0.3649 - acc: 0.8864 - val_loss: 0.1256 - val_acc: 0.9608\n",
            "Epoch 2/75\n",
            "600/600 [==============================] - 22s 37ms/step - loss: 0.1876 - acc: 0.9408 - val_loss: 0.0849 - val_acc: 0.9717\n",
            "Epoch 3/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.1524 - acc: 0.9516 - val_loss: 0.0780 - val_acc: 0.9744\n",
            "Epoch 4/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.1288 - acc: 0.9595 - val_loss: 0.0691 - val_acc: 0.9787\n",
            "Epoch 5/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.1164 - acc: 0.9630 - val_loss: 0.0594 - val_acc: 0.9820\n",
            "Epoch 6/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.1064 - acc: 0.9667 - val_loss: 0.0573 - val_acc: 0.9799\n",
            "Epoch 7/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0975 - acc: 0.9690 - val_loss: 0.0522 - val_acc: 0.9822\n",
            "Epoch 8/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0930 - acc: 0.9700 - val_loss: 0.0493 - val_acc: 0.9831\n",
            "Epoch 9/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0883 - acc: 0.9718 - val_loss: 0.0478 - val_acc: 0.9848\n",
            "Epoch 10/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0814 - acc: 0.9744 - val_loss: 0.0457 - val_acc: 0.9853\n",
            "Epoch 11/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0785 - acc: 0.9753 - val_loss: 0.0478 - val_acc: 0.9833\n",
            "Epoch 12/75\n",
            "600/600 [==============================] - 22s 37ms/step - loss: 0.0740 - acc: 0.9765 - val_loss: 0.0381 - val_acc: 0.9864\n",
            "Epoch 13/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0728 - acc: 0.9770 - val_loss: 0.0410 - val_acc: 0.9873\n",
            "Epoch 14/75\n",
            "600/600 [==============================] - 22s 37ms/step - loss: 0.0684 - acc: 0.9784 - val_loss: 0.0385 - val_acc: 0.9876\n",
            "Epoch 15/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0675 - acc: 0.9791 - val_loss: 0.0397 - val_acc: 0.9871\n",
            "Epoch 16/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0653 - acc: 0.9792 - val_loss: 0.0389 - val_acc: 0.9879\n",
            "Epoch 17/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0626 - acc: 0.9799 - val_loss: 0.0366 - val_acc: 0.9883\n",
            "Epoch 18/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0595 - acc: 0.9803 - val_loss: 0.0311 - val_acc: 0.9896\n",
            "Epoch 19/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0604 - acc: 0.9814 - val_loss: 0.0346 - val_acc: 0.9883\n",
            "Epoch 20/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0573 - acc: 0.9821 - val_loss: 0.0338 - val_acc: 0.9896\n",
            "Epoch 21/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0559 - acc: 0.9822 - val_loss: 0.0354 - val_acc: 0.9881\n",
            "Epoch 22/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0536 - acc: 0.9827 - val_loss: 0.0346 - val_acc: 0.9886\n",
            "Epoch 23/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0538 - acc: 0.9827 - val_loss: 0.0321 - val_acc: 0.9889\n",
            "Epoch 24/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0534 - acc: 0.9824 - val_loss: 0.0354 - val_acc: 0.9882\n",
            "Epoch 25/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0487 - acc: 0.9843 - val_loss: 0.0286 - val_acc: 0.9913\n",
            "Epoch 26/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0434 - acc: 0.9867 - val_loss: 0.0261 - val_acc: 0.9913\n",
            "Epoch 27/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0371 - acc: 0.9880 - val_loss: 0.0249 - val_acc: 0.9915\n",
            "Epoch 28/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0365 - acc: 0.9883 - val_loss: 0.0248 - val_acc: 0.9915\n",
            "Epoch 29/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0366 - acc: 0.9887 - val_loss: 0.0250 - val_acc: 0.9918\n",
            "Epoch 30/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0335 - acc: 0.9895 - val_loss: 0.0247 - val_acc: 0.9915\n",
            "Epoch 31/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0340 - acc: 0.9895 - val_loss: 0.0242 - val_acc: 0.9919\n",
            "Epoch 32/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0334 - acc: 0.9893 - val_loss: 0.0241 - val_acc: 0.9920\n",
            "Epoch 33/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0336 - acc: 0.9894 - val_loss: 0.0246 - val_acc: 0.9918\n",
            "Epoch 34/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0336 - acc: 0.9892 - val_loss: 0.0232 - val_acc: 0.9919\n",
            "Epoch 35/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0317 - acc: 0.9897 - val_loss: 0.0235 - val_acc: 0.9919\n",
            "Epoch 36/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0322 - acc: 0.9899 - val_loss: 0.0236 - val_acc: 0.9925\n",
            "Epoch 37/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0299 - acc: 0.9904 - val_loss: 0.0233 - val_acc: 0.9920\n",
            "Epoch 38/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0319 - acc: 0.9902 - val_loss: 0.0235 - val_acc: 0.9919\n",
            "Epoch 39/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0320 - acc: 0.9897 - val_loss: 0.0232 - val_acc: 0.9922\n",
            "Epoch 40/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0306 - acc: 0.9898 - val_loss: 0.0231 - val_acc: 0.9919\n",
            "Epoch 41/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0305 - acc: 0.9901 - val_loss: 0.0229 - val_acc: 0.9916\n",
            "Epoch 42/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0305 - acc: 0.9901 - val_loss: 0.0232 - val_acc: 0.9921\n",
            "Epoch 43/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0312 - acc: 0.9905 - val_loss: 0.0230 - val_acc: 0.9920\n",
            "Epoch 44/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0293 - acc: 0.9908 - val_loss: 0.0225 - val_acc: 0.9924\n",
            "Epoch 45/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0304 - acc: 0.9902 - val_loss: 0.0227 - val_acc: 0.9925\n",
            "Epoch 46/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0300 - acc: 0.9905 - val_loss: 0.0229 - val_acc: 0.9923\n",
            "Epoch 47/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0304 - acc: 0.9900 - val_loss: 0.0230 - val_acc: 0.9924\n",
            "Epoch 48/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0295 - acc: 0.9904 - val_loss: 0.0228 - val_acc: 0.9926\n",
            "Epoch 49/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0293 - acc: 0.9910 - val_loss: 0.0225 - val_acc: 0.9923\n",
            "Epoch 50/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0299 - acc: 0.9905 - val_loss: 0.0225 - val_acc: 0.9920\n",
            "Epoch 51/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0290 - acc: 0.9904 - val_loss: 0.0225 - val_acc: 0.9921\n",
            "Epoch 52/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0223 - val_acc: 0.9926\n",
            "Epoch 53/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0274 - acc: 0.9914 - val_loss: 0.0224 - val_acc: 0.9922\n",
            "Epoch 54/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0282 - acc: 0.9907 - val_loss: 0.0223 - val_acc: 0.9924\n",
            "Epoch 55/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0276 - acc: 0.9909 - val_loss: 0.0224 - val_acc: 0.9922\n",
            "Epoch 56/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0276 - acc: 0.9914 - val_loss: 0.0222 - val_acc: 0.9920\n",
            "Epoch 57/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0277 - acc: 0.9910 - val_loss: 0.0224 - val_acc: 0.9920\n",
            "Epoch 58/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0295 - acc: 0.9908 - val_loss: 0.0223 - val_acc: 0.9922\n",
            "Epoch 59/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0225 - val_acc: 0.9922\n",
            "Epoch 60/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0267 - acc: 0.9916 - val_loss: 0.0224 - val_acc: 0.9922\n",
            "Epoch 61/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0276 - acc: 0.9911 - val_loss: 0.0224 - val_acc: 0.9926\n",
            "Epoch 62/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0277 - acc: 0.9908 - val_loss: 0.0223 - val_acc: 0.9923\n",
            "Epoch 63/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0269 - acc: 0.9913 - val_loss: 0.0225 - val_acc: 0.9922\n",
            "Epoch 64/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0267 - acc: 0.9919 - val_loss: 0.0224 - val_acc: 0.9924\n",
            "Epoch 65/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0225 - val_acc: 0.9924\n",
            "Epoch 66/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0284 - acc: 0.9908 - val_loss: 0.0224 - val_acc: 0.9925\n",
            "Epoch 67/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0277 - acc: 0.9910 - val_loss: 0.0223 - val_acc: 0.9924\n",
            "Epoch 68/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0275 - acc: 0.9911 - val_loss: 0.0222 - val_acc: 0.9924\n",
            "Epoch 69/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0282 - acc: 0.9909 - val_loss: 0.0221 - val_acc: 0.9924\n",
            "Epoch 70/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0283 - acc: 0.9912 - val_loss: 0.0223 - val_acc: 0.9924\n",
            "Epoch 71/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0293 - acc: 0.9904 - val_loss: 0.0222 - val_acc: 0.9922\n",
            "Epoch 72/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0264 - acc: 0.9918 - val_loss: 0.0222 - val_acc: 0.9924\n",
            "Epoch 73/75\n",
            "600/600 [==============================] - 22s 36ms/step - loss: 0.0275 - acc: 0.9909 - val_loss: 0.0222 - val_acc: 0.9923\n",
            "Epoch 74/75\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0285 - acc: 0.9910 - val_loss: 0.0221 - val_acc: 0.9926\n",
            "Epoch 75/75\n",
            "600/600 [==============================] - 21s 36ms/step - loss: 0.0267 - acc: 0.9914 - val_loss: 0.0221 - val_acc: 0.9925\n",
            "Test loss: 0.022091598324041114\n",
            "Test accuracy: 0.9925\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}